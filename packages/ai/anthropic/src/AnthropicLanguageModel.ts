/**
 * @since 1.0.0
 */
import { AiError } from "@effect/ai/AiError"
import * as AiLanguageModel from "@effect/ai/AiLanguageModel"
import * as AiModel from "@effect/ai/AiModel"
import type * as AiPrompt from "@effect/ai/AiPrompt"
import * as AiResponse from "@effect/ai/AiResponse"
import { addGenAIAnnotations } from "@effect/ai/AiTelemetry"
import type * as Tokenizer from "@effect/ai/Tokenizer"
import * as HttpBody from "@effect/platform/HttpBody"
import * as HttpClientRequest from "@effect/platform/HttpClientRequest"
import * as Arr from "effect/Array"
import * as Context from "effect/Context"
import * as Effect from "effect/Effect"
import * as Encoding from "effect/Encoding"
import { dual, pipe } from "effect/Function"
import * as Layer from "effect/Layer"
import * as Option from "effect/Option"
import * as Predicate from "effect/Predicate"
import * as Schema from "effect/Schema"
import * as Stream from "effect/Stream"
import type { Span } from "effect/Tracer"
import type { DeepMutable, Mutable, Simplify } from "effect/Types"
import { AnthropicClient } from "./AnthropicClient.js"
import * as AnthropicTokenizer from "./AnthropicTokenizer.js"
import type * as Generated from "./Generated.js"
import type { MessageStreamEvent } from "./internal/message-stream-event.js"
import * as InternalUtilities from "./internal/utilities.js"

const constDisableValidation: Schema.MakeOptions = { disableValidation: true } as const

/**
 * @since 1.0.0
 * @category Models
 */
export type Model = typeof Generated.Model.Encoded

// =============================================================================
// Configuration
// =============================================================================

/**
 * @since 1.0.0
 * @category Context
 */
export class Config extends Context.Tag("@effect/ai-anthropic/AnthropicLanguageModel/Config")<
  Config,
  Config.Service
>() {
  /**
   * @since 1.0.0
   */
  static readonly getOrUndefined: Effect.Effect<typeof Config.Service | undefined> = Effect.map(
    Effect.context<never>(),
    (context) => context.unsafeMap.get(Config.key)
  )
}

/**
 * @since 1.0.0
 */
export declare namespace Config {
  /**
   * @since 1.0.0
   * @category Configuration
   */
  export interface Service extends
    Simplify<
      Partial<
        Omit<
          typeof Generated.CreateMessageParams.Encoded,
          "messages" | "tools" | "tool_choice" | "stream"
        >
      >
    >
  {}
}

// =============================================================================
// Anthropic Provider Options / Metadata
// =============================================================================

/**
 * @since 1.0.0
 * @category Provider Metadata
 */
export class ProviderOptions extends Context.Tag(InternalUtilities.ProviderOptionsKey)<
  ProviderOptions,
  ProviderOptions.Service
>() {}

/**
 * @since 1.0.0
 * @category Provider Metadata
 */
export type AnthropicReasoningMetadata = {
  readonly type: "thinking"
  /**
   * Thinking content as an encrypted string, which is used to verify
   * that thinking content was indeed generated by Anthropic's API.
   */
  readonly signature: typeof Generated.ResponseThinkingBlock.fields.thinking.Encoded
} | {
  readonly type: "redacted_thinking"
  /**
   * Thinking content which was flagged by Anthropic's safety systems, and
   * was therefore encrypted.
   */
  readonly redactedData: typeof Generated.RequestRedactedThinkingBlock.fields.data.Encoded
}

/**
 * @since 1.0.0
 * @category Provider Metadata
 */
export declare namespace ProviderOptions {
  /**
   * @since 1.0.0
   * @category Provider Metadata
   */
  export interface Service {
    readonly [AiPrompt.ProviderOptions.UserMessage]: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }

    readonly [AiPrompt.ProviderOptions.AssistantMessage]: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }

    readonly [AiPrompt.ProviderOptions.ToolMessage]: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }

    readonly [AiPrompt.ProviderOptions.TextPart]: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }

    readonly [AiPrompt.ProviderOptions.FilePart]: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
      /**
       * Whether or not citations should be enabled for the file part.
       */
      readonly citations?: typeof Generated.RequestCitationsConfig.Encoded
      /**
       * A custom title to provide to the document. If omitted, the file part's
       * `fileName` property will be used.
       */
      readonly documentTitle?: typeof Generated.RequestDocumentBlock.fields.title.from.Encoded
      /**
       * Additional context about the document that will be forwarded to the
       * large language model, but will not be used towards cited content.
       *
       * Useful for storing additional document metadata as text or stringified JSON.
       */
      readonly documentContext?: typeof Generated.RequestDocumentBlock.fields.context.from.Encoded
    }

    readonly [AiPrompt.ProviderOptions.ReasoningPart]: Simplify<
      AnthropicReasoningMetadata & {
        /**
         * A breakpoint which marks the end of reusable content eligible for caching.
         */
        readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
      }
    >

    readonly [AiPrompt.ProviderOptions.ToolCallPart]: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }

    readonly [AiPrompt.ProviderOptions.ToolCallResultPart]: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }
  }
}

/**
 * @since 1.0.0
 * @category Context
 */
export class ProviderMetadata extends Context.Tag(InternalUtilities.ProviderMetadataKey)<
  ProviderMetadata,
  ProviderMetadata.Service
>() {}

/**
 * @since 1.0.0
 */
export declare namespace ProviderMetadata {
  /**
   * @since 1.0.0
   * @category Provider Metadata
   */
  export interface Service {
    readonly [AiResponse.ProviderMetadata.ReasoningPart]: AnthropicReasoningMetadata

    readonly [AiResponse.ProviderMetadata.FinishPart]: {
      /**
       * Additional usage information provided by the Anthropic API.
       */
      readonly usage: {
        /**
         * The breakdown of cached tokens by TTL.
         */
        readonly cacheCreation: {
          /**
           * The number of input tokens used to create the 5 minute cache entry.
           */
          readonly ephemeral5mInputTokens: number
          /**
           * The number of input tokens used to create the 1 hour cache entry.
           */
          readonly ephemeral1hInputTokens: number
        } | undefined
        /**
         * The number of input tokens utilized to create the cache entry.
         */
        readonly cacheCreationInputTokens: number | undefined
        /**
         * The number of server tool requests.
         */
        readonly serverToolUse: {
          /**
           * The number of web search tool requests.
           */
          readonly webSearchRequests: number
        } | undefined
        /**
         * The service tier the request used, if specified.
         */
        readonly serviceTier: typeof Generated.UsageServiceTierEnum.Encoded | undefined
      }
      /**
       * Which custom stop sequence was generated, if any.
       *
       * If one of the custom user-defined stop sequences was generated, the
       * value will be a `string` with that stop sequence.
       */
      readonly stopSequence: string | undefined
    }

    readonly [AiResponse.ProviderMetadata.DocumentSourcePart]: {
      readonly type: "char_location"
      /**
       * The text that was cited in the response.
       */
      readonly citedText: string
      /**
       * The 0-indexed starting position of the characters that were cited.
       */
      readonly startCharIndex: number
      /**
       * The exclusive ending position of the characters that were cited.
       */
      readonly endCharIndex: number
    } | {
      readonly type: "page_location"
      /**
       * The text that was cited in the response.
       */
      readonly citedText: string
      /**
       * The 1-indexed starting page of pages that were cited.
       */
      readonly startPageNumber: number
      /**
       * The exclusive ending position of the pages that were cited.
       */
      readonly endPageNumber: number
    }

    readonly [AiResponse.ProviderMetadata.UrlSourcePart]: {
      /**
       * Up to 150 characters of the text content that was referenced from the
       * URL source material.
       */
      readonly citedText: string
      /**
       * An internal reference that must be passed back to the Anthropic API
       * during multi-turn conversations.
       */
      readonly encryptedIndex: string
    }
  }
}

// =============================================================================
// Anthropic Language Model
// =============================================================================

/**
 * @since 1.0.0
 * @category AiModels
 */
export const model = (
  model: (string & {}) | Model,
  config?: Omit<Config.Service, "model">
): AiModel.AiModel<AiLanguageModel.AiLanguageModel, AnthropicClient> => AiModel.make(layer({ model, config }))

/**
 * @since 1.0.0
 * @category AiModels
 */
export const modelWithTokenizer = (
  model: (string & {}) | Model,
  config?: Omit<Config.Service, "model">
): AiModel.AiModel<AiLanguageModel.AiLanguageModel | Tokenizer.Tokenizer, AnthropicClient> =>
  AiModel.make(layerWithTokenizer({ model, config }))

/**
 * @since 1.0.0
 * @category Constructors
 */
export const make = Effect.fnUntraced(function*(options: {
  readonly model: (string & {}) | Model
  readonly config?: Omit<Config.Service, "model">
}) {
  const client = yield* AnthropicClient

  const prepareProviderDefinedTool: (
    tool: AiLanguageModel.ProviderDefinedTool
  ) => Effect.Effect<typeof AnthropicAiTool.BuiltInTools.Encoded, AiError> = Effect.fnUntraced(
    function*(tool) {
      switch (tool.id) {
        case "anthropic.web_search_20250305": {
          const args = yield* Schema.encodeUnknown(AnthropicAiTool.WebSearch20250305Args)(tool.args)
          return {
            name: "web_search",
            type: "web_search_20250305",
            ...args
          } satisfies typeof Generated.BetaWebSearchTool20250305.Encoded
        }
        default: {
          const toolStr = JSON.stringify(tool, undefined, 2)
          return yield* Effect.dieMessage(
            `AnthropicLanguageModel.prepareTools: unknown built-in tool detected:\n${toolStr}`
          )
        }
      }
    },
    Effect.mapError((cause) =>
      new AiError({
        module: "AnthropicLanguageModel",
        method: "prepareProviderDefinedTool",
        description: "Failed to encode built-in tool arguments",
        cause
      })
    )
  )

  const prepareUserDefinedTool = (tool: AiLanguageModel.UserDefinedTool): typeof Generated.Tool.Encoded => ({
    name: tool.name,
    description: tool.description,
    input_schema: tool.parameters as any
  })

  const prepareTools = Effect.fnUntraced(function*(options: {
    tools: AiLanguageModel.ProviderOptions["tools"]
    toolChoice: AiLanguageModel.ProviderOptions["toolChoice"]
    useStructured: boolean
  }) {
    if (options.tools.length === 0) {
      return undefined
    }
    if (options.useStructured) {
      const tool = prepareUserDefinedTool(options.tools[0] as AiLanguageModel.UserDefinedTool)
      return [tool]
    }
    const tools: Mutable<typeof Generated.BetaCreateMessageParams.Encoded["tools"]> = []
    for (const tool of options.tools) {
      switch (tool._tag) {
        case "ProviderDefinedTool": {
          tools.push(yield* prepareProviderDefinedTool(tool))
          break
        }
        case "UserDefinedTool": {
          tools.push(prepareUserDefinedTool(tool))
          break
        }
      }
    }
    return tools
  })

  const prepareToolChoice = (options: {
    tools: AiLanguageModel.ProviderOptions["tools"]
    toolChoice: AiLanguageModel.ProviderOptions["toolChoice"]
    useStructured: boolean
  }) => {
    let toolChoice: typeof Generated.ToolChoice.Encoded | undefined = undefined
    if (options.useStructured) {
      toolChoice = { type: "tool", name: options.tools[0].name }
    } else if (options.tools.length > 0) {
      if (options.toolChoice === "required") {
        toolChoice = { type: "any" }
      } else if (typeof options.toolChoice === "object") {
        toolChoice = { type: "tool", name: options.toolChoice.tool }
      } else {
        toolChoice = { type: options.toolChoice }
      }
    }
    return toolChoice
  }

  const makeRequest = Effect.fnUntraced(
    function*(method: string, { prompt, toolChoice, tools }: AiLanguageModel.ProviderOptions) {
      const context = yield* Effect.context<never>()
      const config = { ...options.config, ...context.unsafeMap.get(Config.key) }
      const useStructured = tools.length === 1 && tools[0]._tag === "UserDefinedTool" && tools[0].structured
      const messages = yield* makeMessages(method, prompt)
      return {
        model: options.model,
        max_tokens: 4096,
        ...config,
        messages,
        tools: yield* prepareTools({ tools, toolChoice, useStructured }),
        tool_choice: prepareToolChoice({ tools, toolChoice, useStructured })
      } satisfies typeof Generated.CreateMessageParams.Encoded
    }
  )

  const makeStreamRequest = Effect.fnUntraced(
    function*(method: string, options: AiLanguageModel.ProviderOptions) {
      // Construct the API request
      const request = yield* makeRequest(method, options)
      annotateRequest(options.span, request)
      const httpRequest = HttpClientRequest.post("v1/messages", {
        body: HttpBody.unsafeJson({ ...request, stream: true })
      })

      // Setup all the state we need to resolve the event stream
      const citableDocuments = extractCitableDocuments(options.prompt)
      const usage: Mutable<AiResponse.Usage> = {
        inputTokens: undefined,
        outputTokens: undefined,
        totalTokens: undefined
      }
      let contentBlockType: typeof Generated.ContentBlock.Type["type"] | undefined = undefined
      const contentBlocks: Record<
        number,
        | {
          readonly type: "text"
          readonly text: string
          readonly providerMetadata: Record<string, Record<string, unknown>>
        }
        | {
          readonly type: "reasoning"
          readonly text: string
          readonly providerMetadata: Record<string, Record<string, unknown>>
        }
        | {
          readonly type: "tool-call"
          readonly id: string
          readonly mode: AiResponse.ToolCallPart["mode"]
          readonly name: string
          readonly params: unknown
        }
      > = {}
      let finishReason: AiResponse.FinishReason = "unknown"
      const finishMetadata: DeepMutable<ProviderMetadata.Service[AiResponse.ProviderMetadata.FinishPart]> = {
        stopSequence: undefined,
        usage: {
          cacheCreation: undefined,
          cacheCreationInputTokens: undefined,
          serviceTier: undefined,
          serverToolUse: undefined
        }
      }

      // Resolve the response
      return client.streamRequest<MessageStreamEvent>(httpRequest).pipe(
        Stream.filterMapEffect((event) => {
          const parts: Array<AiResponse.Part> = []

          switch (event.type) {
            case "ping": {
              break
            }

            case "message_start": {
              // Handle standard usage information
              usage.inputTokens = event.message.usage.input_tokens
              if (Predicate.isNotNullable(event.message.usage.cache_read_input_tokens)) {
                usage.cachedInputTokens = event.message.usage.cache_read_input_tokens
              }

              // Handle provider-specific usage information
              if (Predicate.isNotNullable(event.message.usage.cache_creation_input_tokens)) {
                finishMetadata.usage.cacheCreationInputTokens = event.message.usage.cache_creation_input_tokens
              }

              // Add the part to the response
              parts.push(
                new AiResponse.MetadataPart({
                  id: event.message.id,
                  modelId: event.message.model
                }, constDisableValidation)
              )

              break
            }

            case "content_block_start": {
              contentBlockType = event.content_block.type
              switch (event.content_block.type) {
                case "text": {
                  // The text part is always empty when starting a content block
                  break
                }
                case "thinking": {
                  parts.push(
                    new AiResponse.ReasoningPart({
                      text: event.content_block.thinking
                    }, constDisableValidation)
                  )
                  break
                }
                case "redacted_thinking": {
                  let part = new AiResponse.ReasoningPart({
                    text: ""
                  }, constDisableValidation)
                  part = part.setProviderMetadata(ProviderMetadata, {
                    type: "redacted_thinking",
                    redactedData: event.content_block.data
                  })
                  parts.push(part)
                  break
                }
              }
              break
            }

            case "content_block_delta": {
              switch (event.delta.type) {
                case "text_delta": {
                  parts.push(
                    new AiResponse.TextPart({
                      text: event.delta.text
                    }, constDisableValidation)
                  )
                  break
                }
                case "thinking_delta": {
                  parts.push(
                    new AiResponse.ReasoningPart({
                      text: event.delta.thinking
                    }, constDisableValidation)
                  )
                  break
                }
                case "signature_delta": {
                  let part = new AiResponse.ReasoningPart({
                    text: ""
                  }, constDisableValidation)
                  part = part.setProviderMetadata(ProviderMetadata, {
                    type: "thinking",
                    signature: event.delta.signature
                  })
                  parts.push(part)
                  break
                }
                case "input_json_delta": {
                  break
                }
                case "citations_delta": {
                  break
                }
              }
              break
            }

            case "content_block_stop": {
              break
            }

            case "message_delta": {
              usage.outputTokens = event.usage.output_tokens
              usage.totalTokens = (usage.inputTokens ?? 0) + (usage.outputTokens ?? 0)
              if (Predicate.isNotNullable(event.delta.stop_sequence)) {
                finishMetadata.stopSequence = event.delta.stop_sequence
              }
              finishReason = InternalUtilities.resolveFinishReason(event.delta.stop_reason)
              break
            }

            case "message_stop": {
              let part = new AiResponse.FinishPart({
                reason: finishReason,
                usage
              }, constDisableValidation)
              part = part.setProviderMetadata(ProviderMetadata, finishMetadata)
              parts.push(part)
              break
            }
          }

          return Option.some(Effect.succeed(AiResponse.empty))
        })
      )
    }
  )

  return yield* AiLanguageModel.make({
    generateText: Effect.fnUntraced(
      function*(options) {
        const request = yield* makeRequest("generateText", options)
        annotateRequest(options.span, request)
        const rawResponse = yield* client.client.messagesPost({ params: {}, payload: request })
        annotateChatResponse(options.span, rawResponse)
        const response = yield* makeResponse(rawResponse)
        return response
      },
      Effect.catchAll((cause) =>
        AiError.is(cause) ? cause : new AiError({
          module: "AnthropicLanguageModel",
          method: "generateText",
          description: "An error occurred",
          cause
        })
      )
    ),
    streamText(options) {
      return makeRequest("streamText", options).pipe(
        Effect.tap((request) => annotateRequest(options.span, request)),
        Effect.map(streamRequest),
        Stream.unwrap,
        Stream.map((response) => {
          annotateStreamResponse(options.span, response)
          return response
        }),
        Stream.catchAll((cause) =>
          AiError.is(cause) ? Effect.fail(cause) : Effect.fail(
            new AiError({
              module: "AnthropicLanguageModel",
              method: "streamText",
              description: "An error occurred",
              cause
            })
          )
        )
      )
    }
  })
})

/**
 * @since 1.0.0
 * @category Layers
 */
export const layer = (options: {
  readonly model: (string & {}) | Model
  readonly config?: Omit<Config.Service, "model">
}): Layer.Layer<AiLanguageModel.AiLanguageModel, never, AnthropicClient> =>
  Layer.effect(AiLanguageModel.AiLanguageModel, make({ model: options.model, config: options.config }))

/**
 * @since 1.0.0
 * @category Layers
 */
export const layerWithTokenizer = (options: {
  readonly model: (string & {}) | Model
  readonly config?: Omit<Config.Service, "model">
}): Layer.Layer<AiLanguageModel.AiLanguageModel | Tokenizer.Tokenizer, never, AnthropicClient> =>
  Layer.merge(layer(options), AnthropicTokenizer.layer)

/**
 * @since 1.0.0
 * @category Configuration
 */
export const withConfigOverride: {
  (config: Config.Service): <A, E, R>(self: Effect.Effect<A, E, R>) => Effect.Effect<A, E, R>
  <A, E, R>(self: Effect.Effect<A, E, R>, config: Config.Service): Effect.Effect<A, E, R>
} = dual<
  (config: Config.Service) => <A, E, R>(self: Effect.Effect<A, E, R>) => Effect.Effect<A, E, R>,
  <A, E, R>(self: Effect.Effect<A, E, R>, config: Config.Service) => Effect.Effect<A, E, R>
>(2, (self, overrides) =>
  Effect.flatMap(
    Config.getOrUndefined,
    (config) => Effect.provideService(self, Config, { ...config, ...overrides })
  ))

// =============================================================================
// Utilities
// =============================================================================

type ContentGroup = AssistantMessageGroup | UserMessageGroup

interface AssistantMessageGroup {
  readonly type: "assistant"
  readonly messages: Array<AiPrompt.AssistantMessage>
}

interface UserMessageGroup {
  readonly type: "user"
  readonly messages: Array<AiPrompt.ToolMessage | AiPrompt.UserMessage>
}

const groupMessages = (prompt: AiPrompt.AiPrompt): Array<ContentGroup> => {
  const messages: Array<ContentGroup> = []
  let current: ContentGroup | undefined = undefined
  for (const message of prompt.content) {
    switch (message.role) {
      case "assistant": {
        if (current?.type !== "assistant") {
          current = { type: "assistant", messages: [] }
          messages.push(current)
        }
        current.messages.push(message)
        break
      }
      case "tool":
      case "user": {
        if (current?.type !== "user") {
          current = { type: "user", messages: [] }
          messages.push(current)
        }
        current.messages.push(message)
        break
      }
    }
  }
  return messages
}

const makeMessages = Effect.fnUntraced(
  function*(method: string, prompt: AiPrompt.AiPrompt) {
    const messages: Array<typeof Generated.InputMessage.Encoded> = []
    const groups = groupMessages(prompt)
    for (let i = 0; i < groups.length; i++) {
      const group = groups[i]
      const isLastGroup = i === groups.length - 1
      switch (group.type) {
        case "assistant": {
          const content: Array<typeof Generated.InputContentBlock.Encoded> = []
          for (let j = 0; j < group.messages.length; j++) {
            const message = group.messages[j]
            const isLastMessage = j === group.messages.length - 1
            for (let k = 0; k < message.parts.length; k++) {
              const part = message.parts[k]
              const isLastPart = k === message.parts.length - 1
              switch (part._tag) {
                case "ReasoningPart": {
                  content.push({
                    type: "thinking",
                    thinking: part.reasoningText,
                    signature: part.signature!
                  })
                  break
                }
                case "RedactedReasoningPart": {
                  content.push({
                    type: "redacted_thinking",
                    data: part.redactedText
                  })
                  break
                }
                case "TextPart": {
                  content.push({
                    type: "text",
                    text:
                      // Anthropic does not allow trailing whitespace in assistant
                      // content blocks
                      isLastGroup && isLastMessage && isLastPart
                        ? part.text.trim()
                        : part.text
                  })
                  break
                }
                case "ToolCallPart": {
                  content.push({
                    type: "tool_use",
                    id: part.id,
                    name: part.name,
                    input: part.params as any
                  })
                  break
                }
              }
            }
          }
          messages.push({ role: "assistant", content })
          break
        }
        case "user": {
          const content: Array<typeof Generated.InputContentBlock.Encoded> = []
          for (let j = 0; j < group.messages.length; j++) {
            const message = group.messages[j]
            switch (message._tag) {
              case "ToolMessage": {
                for (let k = 0; k < message.parts.length; k++) {
                  const part = message.parts[k]
                  // TODO: support advanced tool result content parts
                  content.push({
                    type: "tool_result",
                    tool_use_id: part.id,
                    content: JSON.stringify(part.result)
                  })
                }
                break
              }
              case "UserMessage": {
                for (let k = 0; k < message.parts.length; k++) {
                  const part = message.parts[k]
                  switch (part._tag) {
                    case "FilePart": {
                      if (Predicate.isUndefined(part.mediaType) || part.mediaType !== "application/pdf") {
                        return yield* new AiError({
                          module: "AnthropicLanguageModel",
                          method,
                          description: "AnthropicLanguageModel only supports PDF file inputs"
                        })
                      }
                      content.push({
                        type: "document",
                        source: {
                          type: "base64",
                          media_type: "application/pdf",
                          data: Encoding.encodeBase64(part.data)
                        }
                      })
                      break
                    }
                    case "FileUrlPart": {
                      content.push({
                        type: "document",
                        source: {
                          type: "url",
                          url: part.url.toString()
                        }
                      })
                      break
                    }
                    case "TextPart": {
                      content.push({
                        type: "text",
                        text: part.text
                      })
                      break
                    }
                    case "ImagePart": {
                      content.push({
                        type: "image",
                        source: {
                          type: "base64",
                          media_type: part.mediaType ?? "image/jpeg" as any,
                          data: Encoding.encodeBase64(part.data)
                        }
                      })
                      break
                    }
                    case "ImageUrlPart": {
                      content.push({
                        type: "image",
                        source: {
                          type: "url",
                          url: part.url.toString()
                        }
                      })
                      break
                    }
                  }
                }
                break
              }
            }
          }
          messages.push({ role: "user", content })
          break
        }
      }
    }
    if (Arr.isNonEmptyReadonlyArray(messages)) {
      return messages
    }
    return yield* new AiError({
      module: "AnthropicLanguageModel",
      method,
      description: "Prompt contained no messages"
    })
  }
)

const makeResponse = Effect.fnUntraced(
  function*(response: Generated.Message) {
    const parts: Array<AiResponse.Part> = []
    parts.push(
      new AiResponse.MetadataPart({
        id: response.id,
        model: response.model
      }, constDisableValidation)
    )
    for (const part of response.content) {
      switch (part.type) {
        case "text": {
          parts.push(
            new AiResponse.TextPart({
              text: part.text
            }, constDisableValidation)
          )
          break
        }
        case "tool_use": {
          parts.push(
            AiResponse.ToolCallPart.fromUnknown({
              id: part.id,
              name: part.name,
              params: part.input
            })
          )
          break
        }
        case "thinking": {
          parts.push(
            new AiResponse.ReasoningPart({
              reasoningText: part.thinking,
              signature: part.signature
            }, constDisableValidation)
          )
          break
        }
        case "redacted_thinking": {
          parts.push(
            new AiResponse.RedactedReasoningPart({
              redactedText: part.data
            }, constDisableValidation)
          )
          break
        }
      }
    }
    const metadata: Mutable<ProviderMetadata.Service> = {}
    if (response.stop_sequence !== null) {
      metadata.stopSequence = response.stop_sequence
    }
    parts.push(
      new AiResponse.FinishPart({
        // Anthropic always returns a non-null `stop_reason` for non-streaming responses
        reason: InternalUtilities.resolveFinishReason(response.stop_reason!),
        usage: new AiResponse.Usage({
          inputTokens: response.usage.input_tokens,
          outputTokens: response.usage.output_tokens,
          totalTokens: response.usage.input_tokens + response.usage.output_tokens,
          reasoningTokens: 0,
          cacheReadInputTokens: response.usage.cache_read_input_tokens ?? 0,
          cacheWriteInputTokens: response.usage.cache_creation_input_tokens ?? 0
        }),
        providerMetadata: { [InternalUtilities.ProviderMetadataKey]: metadata }
      }, constDisableValidation)
    )
    return new AiResponse.AiResponse({
      parts
    }, constDisableValidation)
  }
)

const isCitationPart = (part: AiPrompt.UserMessagePart): part is AiPrompt.FilePart => {
  if (part.type === "file" && (part.mediaType === "application/pdf" || part.mediaType === "text/plain")) {
    const metadata = Option.getOrUndefined(part.getProviderOptions(ProviderOptions))
    return metadata?.citations?.enabled ?? false
  }
  return false
}

const extractCitableDocuments = (prompt: AiPrompt.AiPrompt) => {
  return pipe(
    prompt.content,
    Arr.filter((message) => message.role === "user"),
    Arr.flatMap((message) => message.content),
    Arr.filterMap((part) =>
      isCitationPart(part)
        ? Option.some({
          title: part.filename ?? "Untitled Document",
          filename: part.filename,
          mediaType: part.mediaType
        })
        : Option.none()
    )
  )
}

const annotateRequest = (
  span: Span,
  request: typeof Generated.CreateMessageParams.Encoded
): void => {
  addGenAIAnnotations(span, {
    system: "anthropic",
    operation: { name: "chat" },
    request: {
      model: request.model,
      temperature: request.temperature,
      topK: request.top_k,
      topP: request.top_p,
      maxTokens: request.max_tokens,
      stopSequences: Arr.ensure(request.stop_sequences).filter(
        Predicate.isNotNullable
      )
    }
  })
}

const annotateChatResponse = (
  span: Span,
  response: typeof Generated.Message.Encoded
): void => {
  addGenAIAnnotations(span, {
    response: {
      id: response.id,
      model: response.model,
      finishReasons: response.stop_reason ? [response.stop_reason] : undefined
    },
    usage: {
      inputTokens: response.usage.input_tokens,
      outputTokens: response.usage.output_tokens
    }
  })
}

const annotateStreamResponse = (
  span: Span,
  response: AiResponse.AiResponse
) => {
  const metadataPart = response.parts.find((part) => part.type === "metadata")
  const finishPart = response.parts.find((part) => part.type === "finish")
  addGenAIAnnotations(span, {
    response: {
      id: metadataPart?.id,
      model: metadataPart?.modelId,
      finishReasons: finishPart?.reason ? [finishPart.reason] : undefined
    },
    usage: {
      inputTokens: finishPart?.usage.inputTokens,
      outputTokens: finishPart?.usage.outputTokens
    }
  })
}
