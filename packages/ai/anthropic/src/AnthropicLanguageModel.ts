/**
 * @since 1.0.0
 */
import { AiError } from "@effect/ai/AiError"
import * as AiModel from "@effect/ai/AiModel"
import * as IdGenerator from "@effect/ai/IdGenerator"
import * as LanguageModel from "@effect/ai/LanguageModel"
import * as Prompt from "@effect/ai/Prompt"
import * as Response from "@effect/ai/Response"
import { addGenAIAnnotations } from "@effect/ai/Telemetry"
import * as Tool from "@effect/ai/Tool"
import type { HttpClientError } from "@effect/platform/HttpClientError"
import * as Arr from "effect/Array"
import * as Context from "effect/Context"
import * as Effect from "effect/Effect"
import * as Encoding from "effect/Encoding"
import { dual, identity } from "effect/Function"
import * as Layer from "effect/Layer"
import * as Option from "effect/Option"
import type { ParseError } from "effect/ParseResult"
import * as Predicate from "effect/Predicate"
import * as Stream from "effect/Stream"
import type { Span } from "effect/Tracer"
import type { DeepMutable, Mutable, Simplify } from "effect/Types"
import { AnthropicClient, type MessageStreamEvent } from "./AnthropicClient.js"
import type * as Generated from "./Generated.js"
import * as InternalUtilities from "./internal/utilities.js"

/**
 * @since 1.0.0
 * @category Models
 */
export type Model = typeof Generated.Model.Encoded

// =============================================================================
// Configuration
// =============================================================================

/**
 * @since 1.0.0
 * @category Context
 */
export class Config extends Context.Tag("@effect/ai-anthropic/AnthropicLanguageModel/Config")<
  Config,
  Config.Service
>() {
  /**
   * @since 1.0.0
   */
  static readonly getOrUndefined: Effect.Effect<typeof Config.Service | undefined> = Effect.map(
    Effect.context<never>(),
    (context) => context.unsafeMap.get(Config.key)
  )
}

/**
 * @since 1.0.0
 */
export declare namespace Config {
  /**
   * @since 1.0.0
   * @category Configuration
   */
  export interface Service extends
    Simplify<
      Partial<
        Omit<
          typeof Generated.CreateMessageParams.Encoded,
          "messages" | "tools" | "tool_choice" | "stream"
        >
      >
    >
  {
    readonly disableParallelToolCalls?: boolean
  }
}

// =============================================================================
// Anthropic Provider Options / Metadata
// =============================================================================

/**
 * @since 1.0.0
 * @category Provider Metadata
 */
export class ProviderOptions extends Context.Tag(InternalUtilities.ProviderOptionsKey)<
  ProviderOptions,
  ProviderOptions.Service
>() {}

/**
 * @since 1.0.0
 * @category Provider Metadata
 */
export type AnthropicReasoningMetadata = {
  readonly type: "thinking"
  /**
   * Thinking content as an encrypted string, which is used to verify
   * that thinking content was indeed generated by Anthropic's API.
   */
  readonly signature: typeof Generated.ResponseThinkingBlock.fields.thinking.Encoded
} | {
  readonly type: "redacted_thinking"
  /**
   * Thinking content which was flagged by Anthropic's safety systems, and
   * was therefore encrypted.
   */
  readonly redactedData: typeof Generated.RequestRedactedThinkingBlock.fields.data.Encoded
}

/**
 * @since 1.0.0
 * @category Provider Metadata
 */
export declare namespace ProviderOptions {
  /**
   * @since 1.0.0
   * @category Provider Metadata
   */
  export interface Service extends Prompt.AnyProviderOptions {
    readonly system: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }

    readonly user: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }

    readonly assistant: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }

    readonly tool: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }

    readonly text: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }

    readonly file: {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
      /**
       * Whether or not citations should be enabled for the file part.
       */
      readonly citations?: typeof Generated.RequestCitationsConfig.Encoded
      /**
       * A custom title to provide to the document. If omitted, the file part's
       * `fileName` property will be used.
       */
      readonly documentTitle?: typeof Generated.RequestDocumentBlock.fields.title.from.Encoded
      /**
       * Additional context about the document that will be forwarded to the
       * large language model, but will not be used towards cited content.
       *
       * Useful for storing additional document metadata as text or stringified JSON.
       */
      readonly documentContext?: typeof Generated.RequestDocumentBlock.fields.context.from.Encoded
    }

    readonly reasoning: Simplify<
      AnthropicReasoningMetadata & {
        /**
         * A breakpoint which marks the end of reusable content eligible for caching.
         */
        readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
      }
    >

    readonly "tool-call": {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }

    readonly "tool-result": {
      /**
       * A breakpoint which marks the end of reusable content eligible for caching.
       */
      readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
    }
  }
}

/**
 * @since 1.0.0
 * @category Context
 */
export class ProviderMetadata extends Context.Tag(InternalUtilities.ProviderMetadataKey)<
  ProviderMetadata,
  ProviderMetadata.Service
>() {}

/**
 * @since 1.0.0
 */
export declare namespace ProviderMetadata {
  /**
   * @since 1.0.0
   * @category Provider Metadata
   */
  export interface Service extends Response.AnyProviderMetadata {
    readonly reasoning: AnthropicReasoningMetadata

    readonly "reasoning-start": AnthropicReasoningMetadata

    readonly "reasoning-delta": AnthropicReasoningMetadata

    readonly finish: {
      /**
       * Additional usage information provided by the Anthropic API.
       */
      readonly usage: {
        /**
         * The breakdown of cached tokens by TTL.
         */
        readonly cacheCreation?: {
          /**
           * The number of input tokens used to create the 5 minute cache entry.
           */
          readonly ephemeral5mInputTokens: number
          /**
           * The number of input tokens used to create the 1 hour cache entry.
           */
          readonly ephemeral1hInputTokens: number
        } | undefined
        /**
         * The number of input tokens utilized to create the cache entry.
         */
        readonly cacheCreationInputTokens?: number | undefined
        /**
         * The number of server tool requests.
         */
        readonly serverToolUse?: {
          /**
           * The number of web search tool requests.
           */
          readonly webSearchRequests: number
        } | undefined
        /**
         * The service tier the request used, if specified.
         */
        readonly serviceTier?: typeof Generated.UsageServiceTierEnum.Encoded | undefined
      }
      /**
       * Which custom stop sequence was generated, if any.
       *
       * If one of the custom user-defined stop sequences was generated, the
       * value will be a `string` with that stop sequence.
       */
      readonly stopSequence: string | undefined
    }

    readonly "source": {
      readonly sourceType: "document"
      readonly type: "char_location"
      /**
       * The text that was cited in the response.
       */
      readonly citedText: string
      /**
       * The 0-indexed starting position of the characters that were cited.
       */
      readonly startCharIndex: number
      /**
       * The exclusive ending position of the characters that were cited.
       */
      readonly endCharIndex: number
    } | {
      readonly sourceType: "document"
      readonly type: "page_location"
      /**
       * The text that was cited in the response.
       */
      readonly citedText: string
      /**
       * The 1-indexed starting page of pages that were cited.
       */
      readonly startPageNumber: number
      /**
       * The exclusive ending position of the pages that were cited.
       */
      readonly endPageNumber: number
    } | {
      readonly sourceType: "url"
      /**
       * Up to 150 characters of the text content that was referenced from the
       * URL source material.
       */
      readonly citedText: string
      /**
       * An internal reference that must be passed back to the Anthropic API
       * during multi-turn conversations.
       */
      readonly encryptedIndex: string
    }
  }
}

// =============================================================================
// Anthropic Language Model
// =============================================================================

/**
 * @since 1.0.0
 * @category AiModels
 */
export const model = (
  model: (string & {}) | Model,
  config?: Omit<Config.Service, "model">
): AiModel.AiModel<LanguageModel.LanguageModel, AnthropicClient> => AiModel.make(layer({ model, config }))

// /**
//  * @since 1.0.0
//  * @category AiModels
//  */
// export const modelWithTokenizer = (
//   model: (string & {}) | Model,
//   config?: Omit<Config.Service, "model">
// ): AiModel.AiModel<LanguageModel.LanguageModel | Tokenizer.Tokenizer, AnthropicClient> =>
//   AiModel.make(layerWithTokenizer({ model, config }))

/**
 * @since 1.0.0
 * @category Constructors
 */
export const make = Effect.fnUntraced(function*(options: {
  readonly model: (string & {}) | Model
  readonly config?: Omit<Config.Service, "model">
}) {
  const client = yield* AnthropicClient

  const makeRequest = Effect.fnUntraced(
    function*(method: string, providerOptions: LanguageModel.ProviderOptions) {
      const context = yield* Effect.context<never>()
      const config = { ...options.config, ...context.unsafeMap.get(Config.key) }
      const { prompt } = yield* makeMessages(method, providerOptions.prompt)
      const { betas: _betas, toolChoice, tools } = yield* prepareTools(config, providerOptions)
      return identity<typeof Generated.CreateMessageParams.Encoded>({
        model: options.model,
        max_tokens: 4096,
        ...config,
        system: prompt.system,
        messages: prompt.messages,
        tools,
        tool_choice: toolChoice
      })
    }
  )

  return yield* LanguageModel.make({
    generateText: Effect.fnUntraced(
      function*(options) {
        const request = yield* makeRequest("generateText", options)
        annotateRequest(options.span, request)
        const rawResponse = yield* client.client.messagesPost({ params: {}, payload: request })
        annotateResponse(options.span, rawResponse)
        return yield* makeResponse(rawResponse, options)
      },
      Effect.catchAll((cause) =>
        AiError.is(cause) ? cause : new AiError({
          module: "AnthropicLanguageModel",
          method: "generateText",
          description: "An error occurred",
          cause
        })
      )
    ) as any,

    streamText: (options) =>
      Stream.fromEffect(Effect.gen(function*() {
        const request = yield* makeRequest("streamText", options)
        annotateRequest(options.span, request)
        return client.createMessageStream(request)
      })).pipe(
        Stream.flatMap((stream) => makeStreamResponse(stream, options), { switch: true }),
        Stream.map((response) => {
          annotateStreamResponse(options.span, response)
          return response
        }),
        Stream.catchAll((cause) =>
          AiError.is(cause) ? Effect.fail(cause) : Effect.fail(
            new AiError({
              module: "AnthropicLanguageModel",
              method: "streamText",
              description: "An error occurred",
              cause
            })
          )
        )
      ) as any
  })
})

/**
 * @since 1.0.0
 * @category Layers
 */
export const layer = (options: {
  readonly model: (string & {}) | Model
  readonly config?: Omit<Config.Service, "model">
}): Layer.Layer<LanguageModel.LanguageModel, never, AnthropicClient> =>
  Layer.effect(LanguageModel.LanguageModel, make({ model: options.model, config: options.config }))

// /**
//  * @since 1.0.0
//  * @category Layers
//  */
// export const layerWithTokenizer = (options: {
//   readonly model: (string & {}) | Model
//   readonly config?: Omit<Config.Service, "model">
// }): Layer.Layer<AiLanguageModel.AiLanguageModel | Tokenizer.Tokenizer, never, AnthropicClient> =>
//   Layer.merge(layer(options), AnthropicTokenizer.layer)

/**
 * @since 1.0.0
 * @category Configuration
 */
export const withConfigOverride: {
  (config: Config.Service): <A, E, R>(self: Effect.Effect<A, E, R>) => Effect.Effect<A, E, R>
  <A, E, R>(self: Effect.Effect<A, E, R>, config: Config.Service): Effect.Effect<A, E, R>
} = dual<
  (config: Config.Service) => <A, E, R>(self: Effect.Effect<A, E, R>) => Effect.Effect<A, E, R>,
  <A, E, R>(self: Effect.Effect<A, E, R>, config: Config.Service) => Effect.Effect<A, E, R>
>(2, (self, overrides) =>
  Effect.flatMap(
    Config.getOrUndefined,
    (config) => Effect.provideService(self, Config, { ...config, ...overrides })
  ))

// =============================================================================
// Utilities
// =============================================================================

type ContentGroup = SystemMessageGroup | AssistantMessageGroup | UserMessageGroup

interface SystemMessageGroup {
  readonly type: "system"
  readonly messages: Array<Prompt.SystemMessage>
}

interface AssistantMessageGroup {
  readonly type: "assistant"
  readonly messages: Array<Prompt.AssistantMessage>
}

interface UserMessageGroup {
  readonly type: "user"
  readonly messages: Array<Prompt.ToolMessage | Prompt.UserMessage>
}

const groupMessages = (prompt: Prompt.Prompt): Array<ContentGroup> => {
  const messages: Array<ContentGroup> = []
  let current: ContentGroup | undefined = undefined
  for (const message of prompt.content) {
    switch (message.role) {
      case "system": {
        if (current?.type !== "system") {
          current = { type: "system", messages: [] }
          messages.push(current)
        }
        current.messages.push(message)
        break
      }
      case "assistant": {
        if (current?.type !== "assistant") {
          current = { type: "assistant", messages: [] }
          messages.push(current)
        }
        current.messages.push(message)
        break
      }
      case "tool":
      case "user": {
        if (current?.type !== "user") {
          current = { type: "user", messages: [] }
          messages.push(current)
        }
        current.messages.push(message)
        break
      }
    }
  }
  return messages
}

// =============================================================================
// Prompt Conversion
// =============================================================================

const makeMessages: (method: string, prompt: Prompt.Prompt) => Effect.Effect<
  {
    readonly betas: ReadonlySet<string>
    readonly prompt: {
      readonly system: ReadonlyArray<typeof Generated.RequestTextBlock.Encoded> | undefined
      readonly messages: ReadonlyArray<typeof Generated.InputMessage.Encoded>
    }
  },
  AiError
> = Effect.fnUntraced(
  function*(method: string, prompt: Prompt.Prompt) {
    const betas = new Set<string>()
    const groups = groupMessages(prompt)

    let system: Array<typeof Generated.RequestTextBlock.Encoded> | undefined = undefined
    const messages: Array<typeof Generated.InputMessage.Encoded> = []

    for (let i = 0; i < groups.length; i++) {
      const group = groups[i]
      const isLastGroup = i === groups.length - 1

      switch (group.type) {
        case "system": {
          system = group.messages.map((message) => ({
            type: "text",
            text: message.content,
            cache_control: getCacheControl(message)
          }))
          break
        }

        case "user": {
          const content: Array<typeof Generated.InputContentBlock.Encoded> = []

          for (const message of group.messages) {
            switch (message.role) {
              case "user": {
                for (let j = 0; j < message.content.length; j++) {
                  const part = message.content[j]
                  const isLastPart = j === message.content.length - 1

                  // Attempt to get the cache control from the part first. If
                  // the part does not have cache control defined and we are
                  // evaluating the last part for this message, also check the
                  // message for cache control.
                  const cacheControl = getCacheControl(part) ?? (
                    isLastPart ? getCacheControl(message) : undefined
                  )

                  switch (part.type) {
                    case "text": {
                      content.push({
                        type: "text",
                        text: part.text,
                        cache_control: cacheControl
                      })
                      break
                    }

                    case "file": {
                      if (part.mediaType.startsWith("image/")) {
                        const source = part.data instanceof URL ?
                          {
                            type: "url",
                            url: part.data.toString()
                          } as const :
                          {
                            type: "base64",
                            media_type: part.mediaType === "image/*"
                              ? "image/jpeg"
                              : (part.mediaType as typeof Generated.Base64ImageSourceMediaType.Encoded),
                            data: typeof part.data === "string" ? part.data : Encoding.encodeBase64(part.data)
                          } as const

                        content.push({
                          type: "image",
                          source,
                          cache_control: cacheControl
                        })
                      } else if (part.mediaType === "application/pdf" || part.mediaType === "text/plain") {
                        if (part.mediaType === "application/pdf") {
                          betas.add("pdfs-2024-09-25")
                        }

                        const enableCitations = shouldEnableCitations(part)
                        const documentOptions = getDocumentMetadata(part)

                        const source = part.data instanceof URL
                          ? {
                            type: "url",
                            url: part.data.toString()
                          } as const
                          : part.mediaType === "application/pdf"
                          ? {
                            type: "base64",
                            media_type: "application/pdf",
                            data: typeof part.data === "string"
                              ? part.data
                              : Encoding.encodeBase64(part.data)
                          } as const
                          : {
                            type: "text",
                            media_type: "text/plain",
                            data: typeof part.data === "string"
                              ? part.data
                              : Encoding.encodeBase64(part.data)
                          } as const

                        content.push({
                          type: "document",
                          source,
                          title: documentOptions?.title ?? Option.getOrUndefined(part.fileName),
                          ...(documentOptions?.context ? { context: documentOptions.context } : undefined),
                          ...(enableCitations ? { citations: { enabled: true } } : undefined),
                          cache_control: cacheControl
                        })
                      } else {
                        return yield* new AiError({
                          module: "AnthropicLanguageModel",
                          method,
                          description: `Detected unsupported media type for file: '${part.mediaType}'`
                        })
                      }
                      break
                    }
                  }
                }

                break
              }

              // TODO: advanced tool result content parts
              case "tool": {
                for (const part of message.content) {
                  content.push({
                    type: "tool_result",
                    tool_use_id: part.id,
                    content: JSON.stringify(part.result)
                  })
                }

                break
              }
            }
          }

          messages.push({ role: "user", content })

          break
        }

        case "assistant": {
          const content: Array<typeof Generated.InputContentBlock.Encoded> = []

          for (let j = 0; j < group.messages.length; j++) {
            const message = group.messages[j]
            const isLastMessage = j === group.messages.length - 1

            for (let k = 0; k < message.content.length; k++) {
              const part = message.content[k]
              const isLastPart = k === message.content.length - 1

              // Attempt to get the cache control from the part first. If
              // the part does not have cache control defined and we are
              // evaluating the last part for this message, also check the
              // message for cache control.
              const cacheControl = getCacheControl(part) ?? (
                isLastPart ? getCacheControl(message) : undefined
              )

              switch (part.type) {
                case "text": {
                  content.push({
                    type: "text",
                    // Anthropic does not allow trailing whitespace in assistant
                    // content blocks
                    text: isLastGroup && isLastMessage && isLastPart
                      ? part.text.trim()
                      : part.text
                  })
                  break
                }

                case "reasoning": {
                  const providerOptions = Prompt.getProviderOptions(part, ProviderOptions)
                  if (Predicate.isNotUndefined(providerOptions)) {
                    if (providerOptions.type === "thinking") {
                      content.push({
                        type: "thinking",
                        thinking: part.text,
                        signature: providerOptions.signature
                      })
                    } else {
                      content.push({
                        type: "redacted_thinking",
                        data: providerOptions.redactedData
                      })
                    }
                  }
                  break
                }

                case "tool-call": {
                  if (part.isProviderDefined) {
                    if (part.name === "web_search") {
                      content.push({
                        type: "server_tool_use",
                        id: part.id,
                        name: "web_search",
                        input: part.params as any,
                        cache_control: cacheControl
                      })
                    }
                    // TODO(Max): add support for beta provider-defined tool calls
                    // if (part.name === "code_execution") {
                    // }
                  } else {
                    content.push({
                      type: "tool_use",
                      id: part.id,
                      name: part.name,
                      input: part.params as any,
                      cache_control: cacheControl
                    })
                  }
                  break
                }
              }
            }
          }

          messages.push({ role: "assistant", content })

          break
        }
      }
    }

    return {
      prompt: { system, messages },
      betas
    }
  }
)

// =============================================================================
// Response Conversion
// =============================================================================

const makeResponse: (
  response: Generated.Message,
  options: LanguageModel.ProviderOptions
) => Effect.Effect<Array<Response.Part<any>>, AiError, IdGenerator.IdGenerator> = Effect.fnUntraced(
  function*(response: Generated.Message, options: LanguageModel.ProviderOptions) {
    const idGenerator = yield* IdGenerator.IdGenerator
    const parts: Array<Response.Part<any>> = []
    const citableDocuments = extractCitableDocuments(options.prompt)

    const responseMetadata = Response.responseMetadataPart({
      id: Option.some(response.id),
      modelId: Option.some(response.model),
      timestamp: Option.none()
    })

    parts.push(responseMetadata)

    for (const part of response.content) {
      switch (part.type) {
        case "text": {
          // The text parts should only be added to the response here if the
          // response format is `"text"`. If the response format is `"json"`,
          // then the text parts must instead be added to the response when a
          // tool call is received.
          if (options.responseFormat.type === "text") {
            parts.push(Response.textPart({ text: part.text }))

            if (Predicate.isNotNullable(part.citations)) {
              for (const citation of part.citations) {
                const source = yield* processCitation(citation, citableDocuments, idGenerator)
                if (Predicate.isNotUndefined(source)) {
                  parts.push(source)
                }
              }
            }
          }

          break
        }

        case "thinking": {
          const metadata = {
            type: "thinking",
            signature: part.signature
          } satisfies AnthropicReasoningMetadata

          const reasoning = Response.reasoningPart({
            text: part.thinking
          })

          Response.unsafeSetProviderMetadata(reasoning, ProviderMetadata, metadata)

          parts.push(reasoning)

          break
        }

        case "redacted_thinking": {
          const metadata = {
            type: "redacted_thinking",
            redactedData: part.data
          } satisfies AnthropicReasoningMetadata

          const reasoning = Response.reasoningPart({ text: "" })

          Response.unsafeSetProviderMetadata(reasoning, ProviderMetadata, metadata)

          parts.push(reasoning)

          break
        }

        case "tool_use": {
          // When a `"json"` response format is requested, the JSON that we need
          // will be returned by the tool call injected into the request
          if (options.responseFormat.type === "json") {
            parts.push(Response.textPart({
              text: JSON.stringify(part.input)
            }))
          } else {
            parts.push(Response.toolCallPart({
              id: part.id,
              name: part.name,
              params: part.input,
              isProviderDefined: false
            }))
          }

          break
        }

        case "server_tool_use": {
          // TODO(Max): add support for beta provider-defined tool calls
          if (part.name === "web_search" /*|| part.name === "code_execution"*/) {
            const toolCall = Response.toolCallPart({
              id: part.id,
              name: part.name,
              params: part.input,
              isProviderDefined: true
            })

            parts.push(toolCall)
          }

          break
        }

        // TODO(Max): add support for beta provider-defined tool calls
        // case "code_execution_tool_result": {
        //   break
        // }

        case "web_search_tool_result": {
          const toolName = "web_search"
          const tool = options.tools.find((tool) => tool.name === toolName) as Tool.AnyProviderDefined | undefined

          if (Predicate.isUndefined(tool)) {
            return yield* new AiError({
              module: "AnthropicLanguageModel",
              method: "makeResponse",
              description: `Recevied web search tool result but corresponding tool not present in provided toolkit`
            })
          }

          const result = yield* tool.decodeResult(part.content)

          const toolResultPart = Response.toolResultPart({
            id: part.tool_use_id,
            name: toolName,
            result,
            isProviderDefined: true
          })

          parts.push(toolResultPart)

          break
        }
      }
    }

    // Anthropic always returns a non-null `stop_reason` for non-streaming responses
    const finishReason = InternalUtilities.resolveFinishReason(
      response.stop_reason!,
      options.responseFormat.type === "json"
    )
    const usage = new Response.Usage({
      inputTokens: response.usage.input_tokens,
      outputTokens: response.usage.output_tokens,
      totalTokens: response.usage.input_tokens + response.usage.output_tokens,
      cachedInputTokens: response.usage.cache_read_input_tokens ?? undefined
    })

    const finish = Response.finishPart({ reason: finishReason, usage })
    const metadata = { stopSequence: response.stop_sequence } as const

    Response.unsafeSetProviderMetadata(finish, ProviderMetadata, metadata)

    parts.push(finish)

    return parts
  }
)

const makeStreamResponse: (
  stream: Stream.Stream<MessageStreamEvent, HttpClientError | ParseError, never>,
  options: LanguageModel.ProviderOptions
) => Stream.Stream<
  Response.StreamPart<any>,
  AiError | HttpClientError | ParseError,
  IdGenerator.IdGenerator
> = Effect.fnUntraced(
  function*(stream, options) {
    const idGenerator = yield* IdGenerator.IdGenerator
    const citableDocuments = extractCitableDocuments(options.prompt)

    // Setup all requisite state for the streaming response
    let finishReason: Response.FinishReason = "unknown"
    const contentBlocks: Record<
      number,
      | {
        readonly type: "text"
      }
      | {
        readonly type: "reasoning"
      }
      | {
        readonly type: "tool-call"
        readonly id: string
        readonly name: string
        params: string
        readonly isProviderDefined: boolean
      }
    > = {}
    let blockType:
      | "text"
      | "thinking"
      | "redacted_thinking"
      | "tool_use"
      | "server_tool_use"
      | "web_search_tool_result"
      | "code_execution_tool_result"
      | undefined = undefined
    const usage: Mutable<typeof Response.Usage.Encoded> = {
      inputTokens: undefined,
      outputTokens: undefined,
      totalTokens: undefined
    }
    const metadata: DeepMutable<ProviderMetadata.Service["finish"]> = {
      usage: {},
      stopSequence: undefined
    }

    return stream.pipe(
      Stream.mapEffect(Effect.fnUntraced(function*(event: MessageStreamEvent) {
        const parts: Array<Response.StreamPart<any>> = []

        switch (event.type) {
          case "ping": {
            break
          }

          case "message_start": {
            // Track usage metadata
            usage.inputTokens = event.message.usage.input_tokens
            if (Predicate.isNotNullable(event.message.usage.cache_read_input_tokens)) {
              usage.cachedInputTokens = event.message.usage.cache_read_input_tokens
            }
            if (Predicate.isNotNullable(event.message.usage.service_tier)) {
              metadata.usage.serviceTier = event.message.usage.service_tier
            }
            if (Predicate.isNotNullable(event.message.usage.cache_creation_input_tokens)) {
              metadata.usage.cacheCreationInputTokens = event.message.usage.cache_creation_input_tokens
            }
            if (Predicate.isNotNullable(event.message.usage.cache_creation)) {
              metadata.usage.cacheCreation = {
                ephemeral5mInputTokens: event.message.usage.cache_creation.ephemeral_5m_input_tokens,
                ephemeral1hInputTokens: event.message.usage.cache_creation.ephemeral_1h_input_tokens
              }
            }
            if (Predicate.isNotNullable(event.message.usage.server_tool_use)) {
              metadata.usage.serverToolUse = {
                webSearchRequests: event.message.usage.server_tool_use.web_search_requests
              }
            }

            // Track response metadata
            const responseMetadata = Response.responseMetadataPart({
              id: Option.some(event.message.id),
              modelId: Option.some(event.message.model),
              timestamp: Option.none()
            })

            parts.push(responseMetadata)

            break
          }

          case "message_delta": {
            // Track usage metadata
            if (Predicate.isNotNullable(event.usage.output_tokens)) {
              usage.outputTokens = event.usage.output_tokens
            }
            usage.totalTokens = (usage.inputTokens ?? 0) + (event.usage.output_tokens ?? 0)

            // Track stop sequence metadata
            if (Predicate.isNotNullable(event.delta.stop_sequence)) {
              metadata.stopSequence = event.delta.stop_sequence
            }

            // Track the response finish reason
            if (Predicate.isNotNullable(event.delta.stop_reason)) {
              finishReason = InternalUtilities.resolveFinishReason(event.delta.stop_reason)
            }

            break
          }

          case "message_stop": {
            const finishPart = Response.finishPart({
              reason: finishReason,
              usage: new Response.Usage(usage)
            })

            Response.unsafeSetProviderMetadata(finishPart, ProviderMetadata, metadata)

            parts.push(finishPart)

            break
          }

          case "content_block_start": {
            blockType = event.content_block.type

            switch (event.content_block.type) {
              case "text": {
                contentBlocks[event.index] = { type: "text" }

                parts.push(Response.textStartPart({ id: event.index.toString() }))

                break
              }

              case "thinking": {
                contentBlocks[event.index] = { type: "reasoning" }

                parts.push(Response.reasoningStartPart({ id: event.index.toString() }))

                break
              }

              case "redacted_thinking": {
                contentBlocks[event.index] = { type: "reasoning" }

                const reasoningPart = Response.reasoningStartPart({ id: event.index.toString() })

                const metadata: AnthropicReasoningMetadata = {
                  type: "redacted_thinking",
                  redactedData: event.content_block.data
                }

                Response.unsafeSetProviderMetadata(reasoningPart, ProviderMetadata, metadata)

                parts.push(reasoningPart)

                break
              }

              case "tool_use": {
                contentBlocks[event.index] = {
                  type: "tool-call",
                  id: event.content_block.id,
                  name: event.content_block.name,
                  params: "",
                  isProviderDefined: false
                }

                parts.push(Response.toolParamsStartPart({
                  id: event.content_block.id,
                  name: event.content_block.name,
                  isProviderDefined: false
                }))

                break
              }

              case "server_tool_use": {
                if (
                  // TODO(Max): add support for beta provider-defined tool calls
                  /*event.content_block.name === "code_execution" ||*/
                  event.content_block.name === "web_search"
                ) {
                  contentBlocks[event.index] = {
                    type: "tool-call",
                    id: event.content_block.id,
                    name: event.content_block.name,
                    params: "",
                    isProviderDefined: true
                  }

                  parts.push(Response.toolParamsStartPart({
                    id: event.content_block.id,
                    name: event.content_block.name,
                    isProviderDefined: true
                  }))
                }

                break
              }

              // TODO(Max): add support for beta provider-defined tool calls
              // case "code_execution_tool_result": {
              //   break
              // }

              case "web_search_tool_result": {
                const toolName = "web_search"
                const tool = options.tools.find((tool) => tool.name === toolName) as
                  | Tool.AnyProviderDefined
                  | undefined

                if (Predicate.isUndefined(tool)) {
                  return yield* new AiError({
                    module: "AnthropicLanguageModel",
                    method: "makeResponse",
                    description:
                      `Recevied web search tool result but corresponding tool not present in provided toolkit`
                  })
                }

                const result = yield* tool.decodeResult(event.content_block.content)

                const toolResultPart = Response.toolResultPart({
                  id: event.content_block.tool_use_id,
                  name: toolName,
                  result,
                  isProviderDefined: true
                })

                parts.push(toolResultPart)

                break
              }
            }

            break
          }

          case "content_block_delta": {
            switch (event.delta.type) {
              case "text_delta": {
                parts.push(Response.textDeltaPart({
                  id: event.index.toString(),
                  delta: event.delta.text
                }))

                break
              }

              case "thinking_delta": {
                parts.push(Response.reasoningDeltaPart({
                  id: event.index.toString(),
                  delta: event.delta.thinking
                }))

                break
              }

              case "signature_delta": {
                if (blockType === "thinking") {
                  const part = Response.reasoningDeltaPart({
                    id: event.index.toString(),
                    delta: ""
                  })

                  const metadata: AnthropicReasoningMetadata = {
                    type: "thinking",
                    signature: event.delta.signature
                  }

                  Response.unsafeSetProviderMetadata(part, ProviderMetadata, metadata)

                  parts.push(part)
                }

                break
              }

              case "input_json_delta": {
                const contentBlock = contentBlocks[event.index]
                const delta = event.delta.partial_json

                if (contentBlock.type === "tool-call") {
                  parts.push(Response.toolParamsDeltaPart({
                    id: contentBlock.id,
                    delta
                  }))

                  contentBlock.params += delta
                }

                break
              }

              case "citations_delta": {
                const citation = event.delta.citation

                const source = yield* processCitation(citation, citableDocuments, idGenerator)
                if (Predicate.isNotUndefined(source)) {
                  parts.push(source)
                }
              }
            }

            break
          }

          case "content_block_stop": {
            if (Predicate.isNotNullable(contentBlocks[event.index])) {
              const contentBlock = contentBlocks[event.index]

              switch (contentBlock.type) {
                case "text": {
                  parts.push(Response.textEndPart({ id: event.index.toString() }))
                  break
                }

                case "reasoning": {
                  parts.push(Response.reasoningEndPart({ id: event.index.toString() }))
                  break
                }

                case "tool-call": {
                  parts.push(Response.toolParamsEndPart({
                    id: contentBlock.id
                  }))

                  // If the tool call has no parameters, an empty string is returned
                  const params = contentBlock.params.length === 0 ? "{}" : contentBlock.params

                  parts.push(Response.toolCallPart({
                    id: contentBlock.id,
                    name: contentBlock.name,
                    params: JSON.parse(params),
                    isProviderDefined: contentBlock.isProviderDefined
                  }))

                  break
                }
              }

              delete contentBlocks[event.index]
            }

            blockType = undefined

            break
          }
        }

        return parts
      })),
      Stream.flattenIterables
    )
  },
  Stream.unwrap
)

// =============================================================================
// Telemetry
// =============================================================================

const annotateRequest = (
  span: Span,
  request: typeof Generated.CreateMessageParams.Encoded
): void => {
  addGenAIAnnotations(span, {
    system: "anthropic",
    operation: { name: "chat" },
    request: {
      model: request.model,
      temperature: request.temperature,
      topK: request.top_k,
      topP: request.top_p,
      maxTokens: request.max_tokens,
      stopSequences: Arr.ensure(request.stop_sequences).filter(
        Predicate.isNotNullable
      )
    }
  })
}

const annotateResponse = (
  span: Span,
  response: typeof Generated.Message.Encoded
): void => {
  addGenAIAnnotations(span, {
    response: {
      id: response.id,
      model: response.model,
      finishReasons: response.stop_reason ? [response.stop_reason] : undefined
    },
    usage: {
      inputTokens: response.usage.input_tokens,
      outputTokens: response.usage.output_tokens
    }
  })
}

const annotateStreamResponse = (span: Span, part: Response.StreamPart<any>) => {
  if (part.type === "response-metadata") {
    addGenAIAnnotations(span, {
      response: {
        id: Option.getOrUndefined(part.id),
        model: Option.getOrUndefined(part.modelId)
      }
    })
  }
  if (part.type === "finish") {
    addGenAIAnnotations(span, {
      response: {
        finishReasons: [part.reason]
      },
      usage: {
        inputTokens: part.usage.inputTokens,
        outputTokens: part.usage.outputTokens
      }
    })
  }
}

// =============================================================================
// Tool Calling
// =============================================================================

const prepareTools: (config: Config.Service, options: LanguageModel.ProviderOptions) => Effect.Effect<{
  readonly betas: ReadonlySet<string> | undefined
  readonly tools: ReadonlyArray<typeof Generated.Tool.Encoded> | undefined
  readonly toolChoice: typeof Generated.ToolChoice.Encoded | undefined
}, AiError> = Effect.fnUntraced(function*(config, options) {
  // If a JSON response format was requested, create a tool and force its use
  if (options.responseFormat.type === "json") {
    const name = options.responseFormat.name
    const ast = options.responseFormat.schema.ast
    const jsonResponseTool: typeof Generated.Tool.Encoded = {
      name,
      description: Tool.getDescriptionFromSchemaAst(ast) ?? "Response with a JSON object",
      input_schema: Tool.getJsonSchemaFromSchemaAst(ast) as any
    }
    return {
      betas: undefined,
      tools: [jsonResponseTool],
      toolChoice: { type: "tool", name, disable_parallel_tool_use: true }
    }
  }

  // Return immediately if no tools are in the toolkit or a tool choice of
  // "none" was specified
  if (options.tools.length === 0 || options.toolChoice === "none") {
    return { betas: undefined, tools: undefined, toolChoice: undefined }
  }

  const betas = new Set<string>()
  let tools: Array<typeof Generated.Tool.Encoded> = []
  let toolChoice: typeof Generated.ToolChoice.Encoded | undefined = undefined

  // Convert the tools in the toolkit to the provider-defined format
  for (const tool of options.tools) {
    if (Tool.isUserDefined(tool)) {
      tools.push({
        name: tool.name,
        description: Tool.getDescription(tool as any),
        input_schema: Tool.getJsonSchema(tool as any) as any
      })
    }

    if (Tool.isProviderDefined(tool)) {
      // Ensure that the arguments passed to the provider-defined tool are valid
      const args = yield* tool.encodeArgs
      switch (tool.id) {
        case "anthropic.web_search_20250305": {
          tools.push({ name: "web_search", type: "web_search_20250305", ...args })

          break
        }
        default: {
          return yield* new AiError({
            module: "AnthropicLanguageModel",
            method: "prepareTools",
            description: `Received request to call unknown provider-defined tool '${tool.name}'`
          })
        }
      }
    }
  }

  // Convert the tool choice to the provider-defined format
  if (options.toolChoice === "auto") {
    toolChoice = {
      type: "auto",
      disable_parallel_tool_use: config.disableParallelToolCalls
    }
  } else if (options.toolChoice === "required") {
    toolChoice = {
      type: "any",
      disable_parallel_tool_use: config.disableParallelToolCalls
    }
  } else if ("tool" in options.toolChoice) {
    toolChoice = {
      type: "tool",
      name: options.toolChoice.tool,
      disable_parallel_tool_use: config.disableParallelToolCalls
    }
  } else {
    const allowedTools = new Set(options.toolChoice.oneOf)
    tools = tools.filter((tool) => allowedTools.has(tool.name))
    toolChoice = {
      type: options.toolChoice.mode === "required" ? "any" : "auto",
      disable_parallel_tool_use: config.disableParallelToolCalls
    }
  }

  return { betas, tools, toolChoice }
})

// =============================================================================
// Utilities
// =============================================================================

const isCitationPart = (part: Prompt.UserMessage["content"][number]): part is Prompt.FilePart => {
  if (part.type === "file" && (part.mediaType === "application/pdf" || part.mediaType === "text/plain")) {
    const metadata = Prompt.getProviderOptions(part, ProviderOptions)
    return metadata?.citations?.enabled ?? false
  }
  return false
}

interface CitableDocument {
  readonly title: string
  readonly fileName: string | undefined
  readonly mediaType: string
}

const extractCitableDocuments = (prompt: Prompt.Prompt): ReadonlyArray<CitableDocument> => {
  const citableDocuments: Array<CitableDocument> = []
  for (const message of prompt.content) {
    if (message.role === "user") {
      for (const part of message.content) {
        if (isCitationPart(part)) {
          const fileName = Option.getOrUndefined(part.fileName)
          citableDocuments.push({
            title: fileName ?? "Untitled Document",
            fileName,
            mediaType: part.mediaType
          })
        }
      }
    }
  }
  return citableDocuments
}

const getCacheControl = (
  part:
    | Prompt.SystemMessage
    | Prompt.UserMessage
    | Prompt.AssistantMessage
    | Prompt.ToolMessage
    | Prompt.UserMessage["content"][number]
    | Prompt.AssistantMessage["content"][number]
    | Prompt.ToolMessage["content"][number]
): typeof Generated.CacheControlEphemeral.Encoded | undefined => {
  const providerOptions: {
    readonly cacheControl?: typeof Generated.CacheControlEphemeral.Encoded
  } | undefined = Prompt.getProviderOptions(part, ProviderOptions)
  return providerOptions?.cacheControl
}

const getDocumentMetadata = (part: Prompt.FilePart): {
  readonly title: string | undefined
  readonly context: string | undefined
} | undefined => {
  const providerOptions = Prompt.getProviderOptions(part, ProviderOptions)
  if (Predicate.isNotUndefined(providerOptions)) {
    return {
      title: providerOptions.documentTitle ?? undefined,
      context: providerOptions.documentContext ?? undefined
    }
  }
  return undefined
}

const shouldEnableCitations = (part: Prompt.FilePart): boolean => {
  const providerOptions = Prompt.getProviderOptions(part, ProviderOptions)
  return providerOptions?.citations?.enabled ?? false
}

const processCitation = Effect.fnUntraced(function*(
  citation:
    | Generated.ResponseCharLocationCitation
    | Generated.ResponsePageLocationCitation
    | Generated.ResponseContentBlockLocationCitation
    | Generated.ResponseWebSearchResultLocationCitation
    | Generated.ResponseSearchResultLocationCitation,
  citableDocuments: ReadonlyArray<CitableDocument>,
  idGenerator: IdGenerator.Service
) {
  if (citation.type === "page_location" || citation.type === "char_location") {
    const citedDocument = citableDocuments[citation.document_index]
    if (Predicate.isNotUndefined(citedDocument)) {
      const id = yield* idGenerator.generateId()

      const metadata = citation.type === "char_location"
        ? {
          type: citation.type,
          sourceType: "document",
          citedText: citation.cited_text,
          startCharIndex: citation.start_char_index,
          endCharIndex: citation.end_char_index
        } as const
        : {
          type: citation.type,
          sourceType: "document",
          citedText: citation.cited_text,
          startPageNumber: citation.start_page_number,
          endPageNumber: citation.end_page_number
        } as const

      const source = Response.documentSourcePart({
        id,
        mediaType: citedDocument.mediaType,
        title: citation.document_title ?? citedDocument.title,
        fileName: citedDocument.fileName
      })

      Response.unsafeSetProviderMetadata(source, ProviderMetadata, metadata)

      return source
    }
  }

  if (citation.type === "web_search_result_location") {
    const id = yield* idGenerator.generateId()

    const metadata = {
      sourceType: "url",
      citedText: citation.cited_text,
      encryptedIndex: citation.encrypted_index
    } as const

    const source = Response.urlSourcePart({
      id,
      url: new URL(citation.url),
      title: citation.title ?? "Untitled"
    })

    Response.unsafeSetProviderMetadata(source, ProviderMetadata, metadata)

    return source
  }
})
