diff --git a/packages/ai/openai/src/Generated.ts b/packages/ai/openai/src/Generated.ts
index 5e06e0ff8..6db790217 100644
--- a/packages/ai/openai/src/Generated.ts
+++ b/packages/ai/openai/src/Generated.ts
@@ -374,7 +374,7 @@ export class AssistantSupportedModels extends S.Literal(
  * reasoning effort can result in faster responses and fewer tokens used
  * on reasoning in a response.
  */
-export class ReasoningEffort extends S.Literal("low", "medium", "high") {}
+export class ReasoningEffort extends S.Literal("minimal", "low", "medium", "high") {}
 
 export class CreateAssistantRequest extends S.Class<CreateAssistantRequest>("CreateAssistantRequest")({
   /**
@@ -1229,7 +1229,7 @@ export class ChatCompletionTokenLogprob extends S.Class<ChatCompletionTokenLogpr
  *
  *   When this parameter is set, the response body will include the `service_tier` utilized.
  */
-export class ServiceTier extends S.Literal("auto", "default", "flex") {}
+export class ServiceTier extends S.Literal("auto", "default", "flex", "scale") {}
 
 /**
  * The object type, which is always `chat.completion`.
@@ -1327,7 +1327,7 @@ export class CreateChatCompletionResponse
       /**
        * Log probability information for the choice.
        */
-      "logprobs": S.NullOr(S.Struct({
+      "logprobs": S.optional(S.NullOr(S.Struct({
         /**
          * A list of message content tokens with log probability information.
          */
@@ -1336,7 +1336,7 @@ export class CreateChatCompletionResponse
          * A list of message refusal tokens with log probability information.
          */
         "refusal": S.NullOr(S.Array(ChatCompletionTokenLogprob))
-      }))
+      })))
     })),
     /**
      * The Unix timestamp (in seconds) of when the chat completion was created.
@@ -2065,6 +2065,12 @@ export class CreateChatCompletionRequest extends S.Class<CreateChatCompletionReq
   "model": ModelIdsShared,
   "modalities": S.optionalWith(ResponseModalities, { nullable: true }),
   "reasoning_effort": S.optionalWith(ReasoningEffort, { nullable: true, default: () => "medium" as const }),
+  /**
+   * Constrains the verbosity of the model's response. Lower values will result
+   * in more concise responseswhile higher values will result in more verbose
+   * responses.
+   */
+  "verbosity": S.optionalWith(S.Literal("low", "medium", "high"), { nullable: true, default: () => "medium" as const }),
   /**
    * An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).
    */
@@ -3111,7 +3117,7 @@ export class Eval extends S.Class<Eval>("Eval")({
       EvalPythonGrader,
       EvalScoreModelGrader
     )
-  ).pipe(S.propertySignature, S.withConstructorDefault(() => "eval" as const)),
+  ),
   /**
    * The Unix timestamp (in seconds) for when the eval was created.
    */
