/**
 * @since 1.0.0
 */
import { AiError } from "@effect/ai/AiError"
import type * as AiInput from "@effect/ai/AiInput"
import * as AiLanguageModel from "@effect/ai/AiLanguageModel"
import * as AiModel from "@effect/ai/AiModel"
import * as AiResponse from "@effect/ai/AiResponse"
import type * as Tokenizer from "@effect/ai/Tokenizer"
import * as Arr from "effect/Array"
import * as Context from "effect/Context"
import * as Effect from "effect/Effect"
import * as Encoding from "effect/Encoding"
import { dual } from "effect/Function"
import * as Layer from "effect/Layer"
import * as Option from "effect/Option"
import * as Predicate from "effect/Predicate"
import * as Stream from "effect/Stream"
import type { Span } from "effect/Tracer"
import type { Simplify } from "effect/Types"
import type * as Generated from "./Generated.js"
import { resolveFinishReason } from "./internal/utilities.js"
import * as InternalUtilities from "./internal/utilities.js"
import { OpenAiClient } from "./OpenAiClient.js"
import { addGenAIAnnotations } from "./OpenAiTelemetry.js"
import * as OpenAiTokenizer from "./OpenAiTokenizer.js"

const constDisableValidation = { disableValidation: true } as const

/**
 * @since 1.0.0
 * @category Models
 */
export type Model = typeof Generated.ModelIdsSharedEnum.Encoded

// =============================================================================
// Configuration
// =============================================================================

/**
 * @since 1.0.0
 * @category Context
 */
export class Config extends Context.Tag("@effect/ai-openai/OpenAiLanguageModel/Config")<
  Config,
  Config.Service
>() {
  /**
   * @since 1.0.0
   */
  static readonly getOrUndefined: Effect.Effect<Config.Service | undefined> = Effect.map(
    Effect.context<never>(),
    (context) => context.unsafeMap.get(Config.key)
  )
}

/**
 * @since 1.0.0
 */
export declare namespace Config {
  /**
   * @since 1.0.0
   * @category Models
   */
  export interface Service extends
    Simplify<
      Partial<
        Omit<
          typeof Generated.CreateChatCompletionRequest.Encoded,
          "messages" | "tools" | "tool_choice" | "stream" | "stream_options" | "functions"
        >
      >
    >
  {}
}

// =============================================================================
// Anthropic Provider Metadata
// =============================================================================

/**
 * @since 1.0.0
 * @category Context
 */
export class ProviderMetadata extends Context.Tag(InternalUtilities.ProviderMetadataKey)<
  ProviderMetadata,
  ProviderMetadata.Service
>() {}

/**
 * @since 1.0.0
 */
export declare namespace ProviderMetadata {
  /**
   * @since 1.0.0
   * @category Provider Metadata
   */
  export interface Service {
    /**
     * Specifies the latency tier that was used for processing the request.
     */
    readonly serviceTier?: string
    /**
     * This fingerprint represents the backend configuration that the model
     * executes with.
     *
     * Can be used in conjunction with the seed request parameter to understand
     * when backend changes have been made that might impact determinism.
     */
    readonly systemFingerprint: string
    /**
     * When using predicted outputs, the number of tokens in the prediction
     * that appeared in the completion.
     */
    readonly acceptedPredictionTokens: number
    /**
     * When using predicted outputs, the number of tokens in the prediction
     * that did not appear in the completion. However, like reasoning tokens,
     * these tokens are still counted in the total completion tokens for
     * purposes of billing, output, and context window limits.
     */
    readonly rejectedPredictionTokens: number
    /**
     * Audio tokens present in the prompt.
     */
    readonly inputAudioTokens: number
    /**
     * Audio tokens generated by the model.
     */
    readonly outputAudioTokens: number
  }
}

// =============================================================================
// OpenAi Language Model
// =============================================================================

/**
 * @since 1.0.0
 * @category AiModel
 */
export const model = (
  model: (string & {}) | Model,
  config?: Omit<Config.Service, "model">
): AiModel.AiModel<AiLanguageModel.AiLanguageModel, OpenAiClient> => AiModel.make(layer({ model, config }))

/**
 * @since 1.0.0
 * @category AiModel
 */
export const modelWithTokenizer = (
  model: (string & {}) | Model,
  config?: Omit<Config.Service, "model">
): AiModel.AiModel<AiLanguageModel.AiLanguageModel | Tokenizer.Tokenizer, OpenAiClient> =>
  AiModel.make(layerWithTokenizer({ model, config }))

/**
 * @since 1.0.0
 * @category Constructors
 */
export const make = Effect.fnUntraced(function*(options: {
  readonly model: (string & {}) | Model
  readonly config?: Omit<Config.Service, "model">
}) {
  const client = yield* OpenAiClient

  const makeRequest = Effect.fnUntraced(
    function*(method: string, { prompt, system, toolChoice, tools }: AiLanguageModel.AiLanguageModelOptions) {
      const context = yield* Effect.context<never>()
      const useStructured = tools.length === 1 && tools[0].structured
      let tool_choice: typeof Generated.ChatCompletionToolChoiceOption.Encoded | undefined = undefined
      const hasUnallowedTools = typeof toolChoice === "object" && "oneOf" in toolChoice
      if (Predicate.isNotUndefined(toolChoice) && !useStructured && !hasUnallowedTools && tools.length > 0) {
        if (toolChoice === "auto" || toolChoice === "required") {
          tool_choice = toolChoice
        } else if (typeof toolChoice === "object") {
          tool_choice = { type: "function", function: { name: toolChoice.tool } }
        }
      } else if (hasUnallowedTools && !useStructured) {
        tool_choice = toolChoice.mode ?? "auto"
      }
      const messages = yield* makeMessages(method, system, prompt)
      return {
        model: options.model,
        ...options.config,
        ...context.unsafeMap.get(Config.key),
        messages,
        response_format: useStructured ?
          {
            type: "json_schema",
            json_schema: {
              strict: true,
              name: tools[0].name,
              description: tools[0].description,
              schema: tools[0].parameters as any
            }
          } :
          undefined,
        tools: !useStructured && tools.length > 0 ?
          tools.map((tool) => ({
            type: "function",
            function: {
              name: tool.name,
              description: tool.description,
              parameters: tool.parameters as any,
              strict: true
            }
          })) :
          undefined,
        tool_choice
      } satisfies typeof Generated.CreateChatCompletionRequest.Encoded
    }
  )

  return yield* AiLanguageModel.make({
    generateText: Effect.fnUntraced(
      function*(options) {
        const structuredTool = options.tools.length === 1 && options.tools[0].structured
          ? options.tools[0]
          : undefined
        const request = yield* makeRequest("generateText", options)
        annotateRequest(options.span, request)
        const rawResponse = yield* client.client.createChatCompletion(request)
        annotateChatResponse(options.span, rawResponse)
        const response = yield* makeResponse(rawResponse, "generateText", structuredTool)
        return response
      },
      Effect.catchAll((cause) =>
        AiError.is(cause) ? cause : new AiError({
          module: "OpenAiLanguageModel",
          method: "generateText",
          description: "An error occurred",
          cause
        })
      )
    ),
    streamText(options) {
      return makeRequest("streamText", options).pipe(
        Effect.tap((request) => annotateRequest(options.span, request)),
        Effect.map(client.stream),
        Stream.unwrap,
        Stream.map((response) => {
          annotateStreamResponse(options.span, response)
          return response
        }),
        Stream.catchAll((cause) =>
          AiError.is(cause) ? cause : new AiError({
            module: "OpenAiLanguageModel",
            method: "streamText",
            description: "An error occurred",
            cause
          })
        )
      )
    }
  })
})

/**
 * @since 1.0.0
 * @category Layers
 */
export const layer = (options: {
  readonly model: (string & {}) | Model
  readonly config?: Omit<Config.Service, "model">
}): Layer.Layer<AiLanguageModel.AiLanguageModel, never, OpenAiClient> =>
  Layer.effect(AiLanguageModel.AiLanguageModel, make({ model: options.model, config: options.config }))

/**
 * @since 1.0.0
 * @category Layers
 */
export const layerWithTokenizer = (options: {
  readonly model: (string & {}) | Model
  readonly config?: Omit<Config.Service, "model">
}): Layer.Layer<AiLanguageModel.AiLanguageModel | Tokenizer.Tokenizer, never, OpenAiClient> =>
  Layer.merge(layer(options), OpenAiTokenizer.layer(options))

/**
 * @since 1.0.0
 * @category Configuration
 */
export const withConfigOverride: {
  (overrides: Config.Service): <A, E, R>(self: Effect.Effect<A, E, R>) => Effect.Effect<A, E, R>
  <A, E, R>(self: Effect.Effect<A, E, R>, overrides: Config.Service): Effect.Effect<A, E, R>
} = dual<
  (overrides: Config.Service) => <A, E, R>(self: Effect.Effect<A, E, R>) => Effect.Effect<A, E, R>,
  <A, E, R>(self: Effect.Effect<A, E, R>, overrides: Config.Service) => Effect.Effect<A, E, R>
>(2, (self, overrides) =>
  Effect.flatMap(
    Config.getOrUndefined,
    (config) => Effect.provideService(self, Config, { ...config, ...overrides })
  ))

const makeMessages = Effect.fnUntraced(function*(
  method: string,
  system: Option.Option<string>,
  prompt: AiInput.AiInput
) {
  type Messages = Array<typeof Generated.ChatCompletionRequestMessage.Encoded>
  type UserPart = typeof Generated.ChatCompletionRequestUserMessageContentPart.Encoded
  const messages: Messages = Option.match(system, {
    onNone: () => [],
    onSome: (content) => [{ role: "system", content }]
  })
  for (const message of prompt.messages) {
    switch (message._tag) {
      case "AssistantMessage": {
        let text = ""
        const toolCalls: Array<typeof Generated.ChatCompletionMessageToolCall.Encoded> = []
        for (const part of message.parts) {
          switch (part._tag) {
            case "TextPart": {
              text += part.text
              break
            }
            case "ToolCallPart": {
              toolCalls.push({
                id: part.id,
                type: "function",
                function: {
                  name: part.name,
                  arguments: JSON.stringify(part.params)
                }
              })
              break
            }
          }
        }
        messages.push({
          role: "assistant",
          content: text,
          tool_calls: toolCalls.length > 0 ? toolCalls : undefined
        })

        break
      }
      case "ToolMessage": {
        for (const part of message.parts) {
          messages.push({
            role: "tool",
            tool_call_id: part.id,
            content: JSON.stringify(part.result)
          })
        }
        break
      }
      case "UserMessage": {
        // Handle the case where the message content is just a single piece of text
        if (message.parts.length === 1 && message.parts[0]._tag === "TextPart") {
          messages.push({ role: "user", content: message.parts[0].text })
          break
        }
        const content: Array<UserPart> = []
        for (let index = 0; index < message.parts.length; index++) {
          const part = message.parts[index]
          switch (part._tag) {
            // TODO: review file inputs
            case "FilePart": {
              const data = Encoding.encodeBase64(part.data)
              switch (part.mediaType) {
                case "audio/wav": {
                  content.push({
                    type: "input_audio",
                    input_audio: { data, format: "wav" }
                  })
                  break
                }
                case "audio/mp3":
                case "audio/mpeg": {
                  content.push({
                    type: "input_audio",
                    input_audio: { data, format: "mp3" }
                  })
                  break
                }
                case "application/pdf": {
                  content.push({
                    type: "file",
                    file: {
                      filename: part.name ?? `part-${index}.pdf`,
                      file_data: `data:application/pdf;base64,${data}`
                    }
                  })
                  break
                }
                default: {
                  return yield* new AiError({
                    module: "OpenAiLanguageModel",
                    method: "",
                    description: `OpenAi does not support file inputs of type "${part.mediaType}"`
                  })
                }
              }
              break
            }
            case "FileUrlPart": {
              return yield* new AiError({
                module: "OpenAiLanguageModel",
                method,
                description: "OpenAi does not support file content parts with URL data"
              })
            }
            case "TextPart": {
              content.push({ type: "text", text: part.text })
              break
            }
            case "ImagePart": {
              const mediaType = part.mediaType ?? "image/jpeg"
              const base64 = Encoding.encodeBase64(part.data)
              const url = `data:${mediaType};base64,${base64}`
              content.push({ type: "image_url", image_url: { url } })
              break
            }
            case "ImageUrlPart": {
              // TODO: provider options
              // const detail = part.providerOptions?.openai?.imageDetail as any
              content.push({ type: "image_url", image_url: { url: part.url.toString() } })
            }
          }
        }
        if (Arr.isNonEmptyArray(content)) {
          messages.push({
            role: "user",
            name: message.userName,
            content
          })
        }
        break
      }
    }
  }
  if (Arr.isNonEmptyReadonlyArray(messages)) {
    return messages
  }
  return yield* new AiError({
    module: "OpenAiLanguageModel",
    method,
    description: "Prompt contained no messages"
  })
})

const makeResponse = Effect.fnUntraced(function*(
  response: typeof Generated.CreateChatCompletionResponse.Type,
  method: string,
  structuredTool?: AiLanguageModel.AiLanguageModelOptions["tools"][number]
) {
  const choice = response.choices[0]
  if (Predicate.isUndefined(choice)) {
    return yield* new AiError({
      module: "OpenAiLanguageModel",
      method,
      description: "Could not get response"
    })
  }
  const parts: Array<AiResponse.Part> = []
  parts.push(
    new AiResponse.MetadataPart({
      id: response.id,
      model: response.model,
      // OpenAi returns the `created` time in seconds
      timestamp: new Date(response.created * 1000)
    }, constDisableValidation)
  )
  const finishReason = resolveFinishReason(choice.finish_reason)
  const inputTokens = response.usage?.prompt_tokens ?? 0
  const outputTokens = response.usage?.completion_tokens ?? 0
  const totalTokens = inputTokens + outputTokens
  const metadata: Record<string, unknown> = {}
  if (Predicate.isNotUndefined(response.service_tier)) {
    metadata.serviceTier = response.service_tier
  }
  if (Predicate.isNotUndefined(response.system_fingerprint)) {
    metadata.systemFingerprint = response.system_fingerprint
  }
  if (Predicate.isNotUndefined(response.usage?.completion_tokens_details?.accepted_prediction_tokens)) {
    metadata.acceptedPredictionTokens = response.usage?.completion_tokens_details?.accepted_prediction_tokens ?? 0
  }
  if (Predicate.isNotUndefined(response.usage?.completion_tokens_details?.rejected_prediction_tokens)) {
    metadata.rejectedPredictionTokens = response.usage?.completion_tokens_details?.rejected_prediction_tokens ?? 0
  }
  if (Predicate.isNotUndefined(response.usage?.prompt_tokens_details?.audio_tokens)) {
    metadata.inputAudioTokens = response.usage?.prompt_tokens_details?.audio_tokens ?? 0
  }
  if (Predicate.isNotUndefined(response.usage?.completion_tokens_details?.audio_tokens)) {
    metadata.outputAudioTokens = response.usage?.completion_tokens_details?.audio_tokens ?? 0
  }
  parts.push(
    new AiResponse.FinishPart({
      reason: finishReason,
      usage: new AiResponse.Usage({
        inputTokens,
        outputTokens,
        totalTokens,
        reasoningTokens: response.usage?.completion_tokens_details?.reasoning_tokens ?? 0,
        cacheReadInputTokens: response.usage?.prompt_tokens_details?.cached_tokens ?? 0,
        cacheWriteInputTokens: 0
      }, constDisableValidation),
      providerMetadata: { [InternalUtilities.ProviderMetadataKey]: metadata }
    }, constDisableValidation)
  )
  if (Predicate.isNotNullable(choice.message.content)) {
    parts.push(
      new AiResponse.TextPart({
        text: choice.message.content
      }, constDisableValidation)
    )
  }
  const output = new AiResponse.AiResponse({ parts }, constDisableValidation)
  if (Predicate.isNotUndefined(structuredTool)) {
    return yield* AiResponse.withToolCallsJson(output, [{
      id: response.id,
      name: structuredTool.name,
      params: choice.message.content!
    }])
  }
  if (
    Predicate.isNotUndefined(choice.message.tool_calls) &&
    choice.message.tool_calls.length > 0
  ) {
    return yield* AiResponse.withToolCallsJson(
      output,
      choice.message.tool_calls.map((tool) => ({
        id: tool.id,
        name: tool.function.name,
        params: tool.function.arguments
      }))
    )
  }
  return output
})

const annotateRequest = (
  span: Span,
  request: typeof Generated.CreateChatCompletionRequest.Encoded
): void => {
  addGenAIAnnotations(span, {
    system: "openai",
    operation: { name: "chat" },
    request: {
      model: request.model,
      temperature: request.temperature,
      topP: request.top_p,
      maxTokens: request.max_tokens,
      stopSequences: Arr.ensure(request.stop).filter(Predicate.isNotNullable),
      frequencyPenalty: request.frequency_penalty,
      presencePenalty: request.presence_penalty,
      seed: request.seed
    },
    openai: {
      request: {
        responseFormat: request.response_format?.type,
        serviceTier: request.service_tier
      }
    }
  })
}

const annotateChatResponse = (
  span: Span,
  response: typeof Generated.CreateChatCompletionResponse.Type
): void => {
  addGenAIAnnotations(span, {
    response: {
      id: response.id,
      model: response.model,
      finishReasons: response.choices.map((choice) => choice.finish_reason)
    },
    usage: {
      inputTokens: response.usage?.prompt_tokens,
      outputTokens: response.usage?.completion_tokens
    },
    openai: {
      response: {
        systemFingerprint: response.system_fingerprint,
        serviceTier: response.service_tier
      }
    }
  })
}

const annotateStreamResponse = (
  span: Span,
  response: AiResponse.AiResponse
) => {
  const metadataPart = response.parts.find((part) => part._tag === "MetadataPart")
  const finishPart = response.parts.find((part) => part._tag === "FinishPart")
  const providerMetadata = finishPart?.providerMetadata[ProviderMetadata.key]
  addGenAIAnnotations(span, {
    response: {
      id: metadataPart?.id,
      model: metadataPart?.model,
      finishReasons: finishPart?.reason ? [finishPart.reason] : undefined
    },
    usage: {
      inputTokens: finishPart?.usage.inputTokens,
      outputTokens: finishPart?.usage.outputTokens
    },
    openai: {
      response: {
        serviceTier: providerMetadata?.serviceTier as string | undefined,
        systemFingerprint: providerMetadata?.systemFingerprint as string | undefined
      }
    }
  })
}
